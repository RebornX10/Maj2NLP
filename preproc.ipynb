{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Preprocessing for the questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time as time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.tokenize import ToktokTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag.util import untag\n",
    "import contractions\n",
    "# import pycontractions # Alternative better package for removing contractions\n",
    "from autocorrect import Speller\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dtypes_questions = {'Id':'int32', 'Score': 'int16', 'Title': 'str', 'Body': 'str'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_questions = pd.read_csv('data/Questions.csv', usecols=['Id', 'Score', 'Title', 'Body'], dtype=dtypes_questions, nrows=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_questions[['Title', 'Body']] = df_questions[['Title', 'Body']]\\\n",
    "    .applymap(lambda x: str(x)\\\n",
    "              .encode(\"utf-8\", errors='surrogatepass')\\\n",
    "              .decode(\"ISO-8859-1\", errors='surrogatepass'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove all the questions with a negative score\n",
    "df_questions = df_questions[df_questions['Score'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spell = Speller()\n",
    "token = ToktokTokenizer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "charac = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~0123456789'\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "adjective_tag_list = set(['JJ','JJR', 'JJS', 'RBR', 'RBS']) # List of Adjective's tag from nltk package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 100 entries, 0 to 99\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Id      100 non-null    int32 \n",
      " 1   Score   100 non-null    int16 \n",
      " 2   Title   100 non-null    object\n",
      " 3   Body    100 non-null    object\n",
      "dtypes: int16(1), int32(1), object(2)\n",
      "memory usage: 2.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df_questions.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Now let's remove HTML tags from the questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samuel.adone@cdbdx.biz/Documents/Ynov/M1/NLP/Maj2NLP/venv/lib/python3.9/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Parse question and title then return only the text\n",
    "df_questions['Body'] = df_questions['Body'].apply(lambda x: BeautifulSoup(x, 'html.parser').get_text())\n",
    "df_questions['Title'] = df_questions['Title'].apply(lambda x: BeautifulSoup(x, 'html.parser').get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In many places, (1,2,3) and [1,2,3] can be used interchangeably.\\nWhen should I use one or the other, and why?\\n'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_questions['Body'][13]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### every other things to removes such as \\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r\"\\'\", \"'\", text) # apostrophe characters to whitespace\n",
    "    text = re.sub(r\"\\n\", \" \", text) # newlines to whitespace\n",
    "    text = re.sub(r\"\\xa0\", \" \", text) # non-breakable to whitespace\n",
    "    text = re.sub('\\s+', ' ', text) # more than one whitespace character to a single whitespace\n",
    "    text = text.strip(' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_questions['Title'] = df_questions['Title'].apply(lambda x: clean_text(x))\n",
    "df_questions['Body'] = df_questions['Body'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I\\'m creating an ZIP file with ZipFile in Python 2.5, it works ok so far: import zipfile, os locfile = \"test.txt\" loczip = os.path.splitext (locfile)[0] + \".zip\" zip = zipfile.ZipFile (loczip, \"w\") zip.write (locfile) zip.close() but I couldn\\'t find how to encrypt the files in the ZIP file. I could use system and call PKZIP -s, but I suppose there must be a more \"Pythonic\" way. I\\'m looking for an open source solution.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_questions['Body'][43]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# expand_contractions (i.e \"wasn't\", don't', isn't, 'i've')\n",
    "def expand_contractions(text):\n",
    "    text = contractions.fix(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_questions['Title'] = df_questions['Title'].apply(lambda x: expand_contractions(x))\n",
    "df_questions['Body'] = df_questions['Body'].apply(lambda x: expand_contractions(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the best way to sanitise user input for a Python-based web application? Is there a single function to remove HTML characters and any other necessary characters combinations to prevent an XSS or SQL injection attack?'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_questions['Body'][42]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Spelling Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def autocorrect(text):\n",
    "    words = token.tokenize(text)\n",
    "    words_correct = [spell(w) for w in words]\n",
    "    return ' '.join(map(str, words_correct)) # Return the text untokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_questions['Title'] = df_questions['Title'].apply(lambda x: autocorrect(x))\n",
    "df_questions['Body'] = df_questions['Body'].apply(lambda x: autocorrect(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How do I make a menu that does not require the user to press [ enter ] to make a selection ?'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_questions['Title'][11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_questions.to_csv('data/questions_preprocessed.csv', encoding='utf-8', errors='surrogatepass')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Lowering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_questions['Title'] = df_questions['Title'].str.lower()\n",
    "df_questions['Body'] = df_questions['Body'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_questions['Body'][11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### remove all non alphabetical characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def remove_punctuation_and_number(text):\n",
    "    \"\"\"remove all punctuation and number\"\"\"\n",
    "    return text.translate(str.maketrans(\" \", \" \", charac))\n",
    "\n",
    "\n",
    "\n",
    "def remove_non_alphabetical_character(text):\n",
    "    \"\"\"remove all non-alphabetical character\"\"\"\n",
    "    text = re.sub(\"[^a-z]+\", \" \", text) # remove all non-alphabetical character\n",
    "    text = re.sub(\"\\s+\", \" \", text) # remove whitespaces left after the last operation\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_questions['Title'] = df_questions['Title'].apply(lambda x: remove_non_alphabetical_character(x))\n",
    "df_questions['Body'] = df_questions['Body'].apply(lambda x: remove_non_alphabetical_character(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' ow do make a menu that does not require the user to press enter to make a selection '"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_questions['Title'][11]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    words = token.tokenize(text)\n",
    "    filtered = [w for w in words if not w in stop_words]\n",
    "\n",
    "    return ' '.join(map(str, filtered)) # Return the text untokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_questions['Title'] = df_questions['Title'].apply(lambda x: remove_stopwords(x))\n",
    "df_questions['Body'] = df_questions['Body'].apply(lambda x: remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Can help wth calculations: smaller vocabulary\n",
    "make verbs go back to the radical root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def stem_text(text):\n",
    "    words = nltk.word_tokenize(text) # tokenize the text then return a list of tuple (token, nltk_tag)\n",
    "    stem_text = []\n",
    "    for word in words:\n",
    "        stem_text.append(stemmer.stem(word)) # Stem every words\n",
    "    return \" \".join(stem_text) # Return the text untokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_questions['Title'] = df_questions['Title'].apply(lambda x: stem_text(x))\n",
    "df_questions['Body'] = df_questions['Body'].apply(lambda x: stem_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    tokens_tagged = nltk.pos_tag(nltk.word_tokenize(text))\n",
    "    lemmatized_text = []\n",
    "    for word, tag in tokens_tagged:\n",
    "        if tag.startswith('J'):\n",
    "            lemmatized_text.append(lemmatizer.lemmatize(word,'a')) # adjectives\n",
    "        elif tag.startswith('V'):\n",
    "            lemmatized_text.append(lemmatizer.lemmatize(word,'v')) # verbs\n",
    "        elif tag.startswith('N'):\n",
    "            lemmatized_text.append(lemmatizer.lemmatize(word,'n')) # nouns\n",
    "        elif tag.startswith('R'):\n",
    "            lemmatized_text.append(lemmatizer.lemmatize(word,'r')) # adverbs\n",
    "        else:\n",
    "            lemmatized_text.append(lemmatizer.lemmatize(word)) # If no tags found.\n",
    "    return \" \".join(lemmatized_text) # Return the text untokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/samuel.adone@cdbdx.biz/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/samuel.adone@cdbdx.biz/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_questions['Title'] = df_questions['Title'].apply(lambda x:lemmatize_text(x))\n",
    "df_questions['Body'] = df_questions['Body'].apply(lambda x: lemmatize_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_questions['Text'] = df_questions['Title'] + ' ' + df_questions['Body']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Export to CSV"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
