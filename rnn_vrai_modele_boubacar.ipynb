{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions=pd.read_csv('../Maj2NLP/data/Questions.csv', encoding='ISO-8859-1')\n",
    "df_answers=pd.read_csv('../Maj2NLP/data/Answers.csv', encoding='ISO-8859-1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df_questions.merge(df_answers,left_on=\"Id\",right_on=\"ParentId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[[\"Title\",\"Body_y\"]]\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"Title\"],df[\"Body_y\"], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " 113/4936 [..............................] - ETA: 30:38 - loss: 7478236.5000 - accuracy: 0.0259"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node 'RMSprop/RMSprop/update/ReadVariableOp' defined at (most recent call last):\n    File \"c:\\Users\\bouba\\anaconda3\\envs\\chatbot\\lib\\runpy.py\", line 193, in _run_module_as_main\n      \"__main__\", mod_spec)\n    File \"c:\\Users\\bouba\\anaconda3\\envs\\chatbot\\lib\\runpy.py\", line 85, in _run_code\n      exec(code, run_globals)\n    File \"c:\\Users\\bouba\\anaconda3\\envs\\chatbot\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"c:\\Users\\bouba\\anaconda3\\envs\\chatbot\\lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n      app.start()\n    File \"c:\\Users\\bouba\\anaconda3\\envs\\chatbot\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"c:\\Users\\bouba\\anaconda3\\envs\\chatbot\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\Users\\bouba\\anaconda3\\envs\\chatbot\\lib\\asyncio\\base_events.py\", line 541, in run_forever\n      self._run_once()\n    File \"c:\\Users\\bouba\\anaconda3\\envs\\chatbot\\lib\\asyncio\\base_events.py\", line 1786, in _run_once\n      handle._run()\n    File \"c:\\Users\\bouba\\anaconda3\\envs\\chatbot\\lib\\asyncio\\events.py\", line 88, in _run\n      self._context.run(self._callback, *self._args)\n    File \"c:\\Users\\bouba\\anaconda3\\envs\\chatbot\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"c:\\Users\\bouba\\anaconda3\\envs\\chatbot\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"c:\\Users\\bouba\\anaconda3\\envs\\chatbot\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"c:\\Users\\bouba\\anaconda3\\envs\\chatbot\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"c:\\Users\\bouba\\anaconda3\\envs\\chatbot\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 390, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"c:\\Users\\bouba\\anaconda3\\envs\\chatbot\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"c:\\Users\\bouba\\anaconda3\\envs\\chatbot\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2915, in run_cell\n      raw_cell, store_history, silent, shell_futures)\n    File \"c:\\Users\\bouba\\anaconda3\\envs\\chatbot\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2960, in _run_cell\n      return runner(coro)\n    File \"c:\\Users\\bouba\\anaconda3\\envs\\chatbot\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 78, in _pseudo_sync_runner\n      coro.send(None)\n    File \"c:\\Users\\bouba\\anaconda3\\envs\\chatbot\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3186, in run_cell_async\n      interactivity=interactivity, compiler=compiler, result=result)\n    File \"c:\\Users\\bouba\\anaconda3\\envs\\chatbot\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3377, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"c:\\Users\\bouba\\anaconda3\\envs\\chatbot\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3457, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\bouba\\AppData\\Local\\Temp\\ipykernel_47816\\3924152357.py\", line 29, in <module>\n      model.fit(question_sequences, answer_sequences, batch_size=128, epochs=100, validation_split=0.2)\n    File \"c:\\Users\\bouba\\anaconda3\\envs\\chatbot\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\bouba\\anaconda3\\envs\\chatbot\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"c:\\Users\\bouba\\anaconda3\\envs\\chatbot\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"c:\\Users\\bouba\\anaconda3\\envs\\chatbot\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\bouba\\anaconda3\\envs\\chatbot\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"c:\\Users\\bouba\\anaconda3\\envs\\chatbot\\lib\\site-packages\\keras\\engine\\training.py\", line 997, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"c:\\Users\\bouba\\anaconda3\\envs\\chatbot\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 579, in minimize\n      return self.apply_gradients(grads_and_vars, name=name)\n    File \"c:\\Users\\bouba\\anaconda3\\envs\\chatbot\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 744, in apply_gradients\n      name=name,\n    File \"c:\\Users\\bouba\\anaconda3\\envs\\chatbot\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 801, in _distributed_apply\n      group=False,\n    File \"c:\\Users\\bouba\\anaconda3\\envs\\chatbot\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 771, in apply_grad_to_update_var\n      grad.values, var, grad.indices, **apply_kwargs\n    File \"c:\\Users\\bouba\\anaconda3\\envs\\chatbot\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 1454, in _resource_apply_sparse_duplicate_indices\n      summed_grad, handle, unique_indices, **kwargs\n    File \"c:\\Users\\bouba\\anaconda3\\envs\\chatbot\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\rmsprop.py\", line 288, in _resource_apply_sparse\n      rms, rms * coefficients[\"rho\"], use_locking=self._use_locking\nNode: 'RMSprop/RMSprop/update/ReadVariableOp'\nOOM when allocating tensor with shape[962545,128] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[{{node RMSprop/RMSprop/update/ReadVariableOp}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_10192]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_47816\\3924152357.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer_sequences\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mRMSprop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquestion_sequences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manswer_sequences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\bouba\\anaconda3\\envs\\chatbot\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\bouba\\anaconda3\\envs\\chatbot\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 55\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node 'RMSprop/RMSprop/update/ReadVariableOp' defined at (most recent call last):\n    File \"c:\\Users\\bouba\\anaconda3\\envs\\chatbot\\lib\\runpy.py\", line 193, in _run_module_as_main\n      \"__main__\", mod_spec)\n    File \"c:\\Users\\bouba\\anaconda3\\envs\\chatbot\\lib\\runpy.py\", line 85, in _run_code\n      exec(code, run_globals)\n    File \"c:\\Users\\bouba\\anaconda3\\envs\\chatbot\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"c:\\Users\\bouba\\anaconda3\\envs\\chatbot\\lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n      app.start()\n    File \"c:\\Users\\bouba\\anaconda3\\envs\\chatbot\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"c:\\Users\\bouba\\anaconda3\\envs\\chatbot\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\Users\\bouba\\anaconda3\\envs\\chatbot\\lib\\asyncio\\base_events.py\", line 541, in run_forever\n      self._run_once()\n    File \"c:\\Users\\bouba\\anaconda3\\envs\\chatbot\\lib\\asyncio\\base_events.py\", line 1786, in _run_once\n      handle._run()\n    File \"c:\\Users\\bouba\\anaconda3\\envs\\chatbot\\lib\\asyncio\\events.py\", line 88, in _run\n      self._context.run(self._callback, *self._args)\n    File \"c:\\Users\\bouba\\anaconda3\\envs\\chatbot\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"c:\\Users\\bouba\\anaconda3\\envs\\chatbot\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"c:\\Users\\bouba\\anaconda3\\envs\\chatbot\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"c:\\Users\\bouba\\anaconda3\\envs\\chatbot\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"c:\\Users\\bouba\\anaconda3\\envs\\chatbot\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 390, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"c:\\Users\\bouba\\anaconda3\\envs\\chatbot\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"c:\\Users\\bouba\\anaconda3\\envs\\chatbot\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2915, in run_cell\n      raw_cell, store_history, silent, shell_futures)\n    File \"c:\\Users\\bouba\\anaconda3\\envs\\chatbot\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2960, in _run_cell\n      return runner(coro)\n    File \"c:\\Users\\bouba\\anaconda3\\envs\\chatbot\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 78, in _pseudo_sync_runner\n      coro.send(None)\n    File \"c:\\Users\\bouba\\anaconda3\\envs\\chatbot\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3186, in run_cell_async\n      interactivity=interactivity, compiler=compiler, result=result)\n    File \"c:\\Users\\bouba\\anaconda3\\envs\\chatbot\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3377, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"c:\\Users\\bouba\\anaconda3\\envs\\chatbot\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3457, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\bouba\\AppData\\Local\\Temp\\ipykernel_47816\\3924152357.py\", line 29, in <module>\n      model.fit(question_sequences, answer_sequences, batch_size=128, epochs=100, validation_split=0.2)\n    File \"c:\\Users\\bouba\\anaconda3\\envs\\chatbot\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\bouba\\anaconda3\\envs\\chatbot\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"c:\\Users\\bouba\\anaconda3\\envs\\chatbot\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"c:\\Users\\bouba\\anaconda3\\envs\\chatbot\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\bouba\\anaconda3\\envs\\chatbot\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"c:\\Users\\bouba\\anaconda3\\envs\\chatbot\\lib\\site-packages\\keras\\engine\\training.py\", line 997, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"c:\\Users\\bouba\\anaconda3\\envs\\chatbot\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 579, in minimize\n      return self.apply_gradients(grads_and_vars, name=name)\n    File \"c:\\Users\\bouba\\anaconda3\\envs\\chatbot\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 744, in apply_gradients\n      name=name,\n    File \"c:\\Users\\bouba\\anaconda3\\envs\\chatbot\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 801, in _distributed_apply\n      group=False,\n    File \"c:\\Users\\bouba\\anaconda3\\envs\\chatbot\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 771, in apply_grad_to_update_var\n      grad.values, var, grad.indices, **apply_kwargs\n    File \"c:\\Users\\bouba\\anaconda3\\envs\\chatbot\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 1454, in _resource_apply_sparse_duplicate_indices\n      summed_grad, handle, unique_indices, **kwargs\n    File \"c:\\Users\\bouba\\anaconda3\\envs\\chatbot\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\rmsprop.py\", line 288, in _resource_apply_sparse\n      rms, rms * coefficients[\"rho\"], use_locking=self._use_locking\nNode: 'RMSprop/RMSprop/update/ReadVariableOp'\nOOM when allocating tensor with shape[962545,128] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[{{node RMSprop/RMSprop/update/ReadVariableOp}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_10192]"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Charger les données de questions et de réponses\n",
    "questions = X_train\n",
    "answers = y_train\n",
    "\n",
    "# Prétraitement des données\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(questions + answers)\n",
    "\n",
    "# Convertir les questions et les réponses en séquences numériques\n",
    "question_sequences = tokenizer.texts_to_sequences(questions)\n",
    "answer_sequences = tokenizer.texts_to_sequences(answers)\n",
    "\n",
    "# Appliquer le padding pour que toutes les séquences aient la même longueur\n",
    "question_sequences = pad_sequences(question_sequences)\n",
    "answer_sequences = pad_sequences(answer_sequences)\n",
    "\n",
    "# Entraîner le modèle RNN\n",
    "embedding_size = 128\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=embedding_size, input_length=question_sequences.shape[1]))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dense(answer_sequences.shape[1], activation='relu'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])\n",
    "model.fit(question_sequences, answer_sequences, batch_size=128, epochs=100, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24928\\534617329.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Prétraitement de la question\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mquestion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Votre question ?\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mquestion_sequence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mquestion\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mquestion_sequence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquestion_sequence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mquestion_sequences\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\bouba\\anaconda3\\envs\\chatbot\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"shell\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_parent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"shell\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1181\u001b[1;33m             \u001b[0mpassword\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1182\u001b[0m         )\n\u001b[0;32m   1183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\bouba\\anaconda3\\envs\\chatbot\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1217\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m                 \u001b[1;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1219\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Interrupted by user\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1220\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid Message:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "# Prétraitement de la question\n",
    "question = input(\"Votre question ?\")\n",
    "question_sequence = tokenizer.texts_to_sequences([question])\n",
    "question_sequence = pad_sequences(question_sequence, maxlen=question_sequences.shape[1])\n",
    "\n",
    "# Prédiction de la réponse\n",
    "predicted_answer = model.predict(question_sequence)\n",
    "print(tokenizer.sequences_to_texts(predicted_answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "03041b0ab7cd1638a53e9defcca4554f53cc9626e5c5fef97f44058438290e8d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
